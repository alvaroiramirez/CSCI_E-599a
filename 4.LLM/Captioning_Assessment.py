"""
Captioning Assessment Script
============================

This script evaluates predicted binary labels against the LADIv2 dataset ground truth
and generates visual and quantitative comparison outputs in Excel and console. The predicted
binary labels are generated by LLM_Captioning_Raw.py.

The Excel file contains the following sheets:
   - Comparison: Shows true, predicted, and match status per label per image. This is the most
        detailed data in the process. 
   - Metrics: This table shows the TP, TN, FP, FN, accuracy, precision, recall, and F1-score 
        calculated on all images.
   - Summary: Token and cost statistics per subfolder and total.
"""

import os
import time
import json
import re
import pandas as pd
from pathlib import Path
from datetime import datetime
from rich.console import Console
from rich.progress import Progress
from rich.table import Table
from openpyxl import Workbook
from openpyxl.styles import PatternFill, Alignment, numbers, Border, Side
from openpyxl.utils import get_column_letter
from sklearn.metrics import precision_score, recall_score, f1_score, hamming_loss, accuracy_score


# Console setup
console = Console()
os.system("clear")
console.print()


# CONSTANTS
# Cost per token
COST_PER_TOKEN = 0.000005

# Project path
project_root = Path(__file__).parent.parent

# Dataset folder
subfolder_name = "LADIv2"

# LADI v2 - Ground truth CSV
ladi_csv_path = project_root / "LADIv2.csv"

# Results folder
results_csv_path = project_root / subfolder_name
results_csv_path = results_csv_path / "results"

# Files
pred_file = results_csv_path / "results.csv"
output_excel_path = results_csv_path / "comparison.xlsx"


# DATA LOADING
# Load ground truth metadata
console.print("Loading LADIv2.csv...", style="bold blue")
df_ladi = pd.read_csv(ladi_csv_path)

# Load predictions
console.print(f"Loading predictions from {pred_file.name}...", style="bold blue")
df_pred = pd.read_csv(pred_file)


# Merge 
merged = pd.merge(df_pred, df_ladi, on="filename", suffixes=("_pred", "_true"))


# METRICS
accuracy = {}

start_time = time.time()
subfolder_start = time.time()

df_comparison = pd.DataFrame(columns=[col.replace("_true", "") for col in merged.columns if col.endswith("_true")])

#os.system("clear")

with Progress(console=console, transient=False) as progress:
    task = progress.add_task("[green]Comparing predictions...", total=len(merged))
    for idx, row in merged.iterrows():
        pred_values = [row.get(f"{col}_pred", "") for col in df_comparison.columns]
        true_values = [row.get(f"{col}_true", "") for col in df_comparison.columns]
        row_comp = []
        for col in df_comparison.columns:
            val_pred = int(str(row.get(f"{col}_pred", "")).strip().lower() in ("1", "true"))
            val_true = int(str(row.get(f"{col}_true", "")).strip().lower() in ("1", "true"))
            row_comp.append(int(val_pred == val_true))
        df_comparison.loc[row["filename"]] = row_comp
        progress.update(task, advance=1)
    progress.stop()
    console.print()

for col in df_comparison.columns:
    accuracy[col] = df_comparison[col].mean()

metrics = {
    "TP": [],
    "TN": [],
    "FP": [],
    "FN": [],
    "Precision": [],
    "Recall": [],
    "F1-score": []
}
labels = df_comparison.columns.tolist()

for col in labels:
    y_true = merged[f"{col}_true"].apply(lambda x: int(str(x).strip().lower() in ("1", "true"))).values
    y_pred = merged[f"{col}_pred"].apply(lambda x: int(str(x).strip().lower() in ("1", "true"))).values

    TP = ((y_pred == 1) & (y_true == 1)).sum()
    TN = ((y_pred == 0) & (y_true == 0)).sum()
    FP = ((y_pred == 1) & (y_true == 0)).sum()
    FN = ((y_pred == 0) & (y_true == 1)).sum()

    precision = precision_score(y_true, y_pred, zero_division=0)
    recall = recall_score(y_true, y_pred, zero_division=0)
    f1 = f1_score(y_true, y_pred, zero_division=0)

    metrics["TP"].append(TP)
    metrics["TN"].append(TN)
    metrics["FP"].append(FP)
    metrics["FN"].append(FN)
    metrics["Precision"].append(precision)
    metrics["Recall"].append(recall)
    metrics["F1-score"].append(f1)

df_metrics = pd.DataFrame(metrics, index=labels)

# Insert Accuracy column into df_metrics
df_metrics.insert(0, "Accuracy", [accuracy[col] for col in df_metrics.index])

# Compute exact match ratio and Hamming loss
y_true_matrix = merged[[f"{col}_true" for col in labels]].apply(lambda col: col.map(lambda x: int(str(x).strip().lower() in ("1", "true")))).values
y_pred_matrix = merged[[f"{col}_pred" for col in labels]].apply(lambda col: col.map(lambda x: int(str(x).strip().lower() in ("1", "true")))).values

exact_match_ratio = accuracy_score(y_true_matrix, y_pred_matrix)
hamming = hamming_loss(y_true_matrix, y_pred_matrix)


# EXPORT TO EXCEL
console.print()
console.print(f"Saving results to Excel: {output_excel_path.name}", style="bold green")

# Comparison sheet
comparison_rows = []
comparison_columns = ["filename"]
comparison_headers = []

for col in labels:
    true_col = f"{col}_true"
    pred_col = f"{col}_pred"
    comp_col = f"{col}_comp"

    comparison_columns.extend([true_col, pred_col, comp_col])
    comparison_headers.extend([
        f"{col.replace('_', ' ').title()} (True)",
        f"{col.replace('_', ' ').title()} (Pred)",
        f"{col.replace('_', ' ').title()} (Comp)"
    ])

for _, row in merged.iterrows():
    data = [row["filename"]]
    for col in labels:
        val_true = int(str(row[f"{col}_true"]).strip().lower() in ("1", "true"))
        val_pred = int(str(row[f"{col}_pred"]).strip().lower() in ("1", "true"))
        val_comp = int(val_true == val_pred)
        data.extend([val_true, val_pred, val_comp])
    comparison_rows.append(data)

df_combined = pd.DataFrame(comparison_rows, columns=["filename"] + comparison_headers)

with pd.ExcelWriter(output_excel_path, engine="openpyxl") as writer:
    wb = writer.book
    ws = wb.create_sheet(title="Comparison", index=0)
    red_fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")
    green_fill = PatternFill(start_color="C6EFCE", end_color="C6EFCE", fill_type="solid")

    row1_headers = ["Filename"]
    row2_headers = [""]
    for col in labels:
        row1_headers.extend([col.replace("_", " ").title()] + ["", ""])
        row2_headers.extend(["True", "Pred", "Comp"])

    for col_idx, header in enumerate(row1_headers, 1):
        ws.cell(row=1, column=col_idx, value=header)
    for col_idx, header in enumerate(row2_headers, 1):
        ws.cell(row=2, column=col_idx, value=header)

    col_index = 2
    for _ in labels:
        ws.merge_cells(start_row=1, start_column=col_index, end_row=1, end_column=col_index+2)
        col_index += 3

    ws.merge_cells(start_row=1, start_column=1, end_row=2, end_column=1)

    for row_idx, data_row in enumerate(df_combined.values, start=3):
        for col_idx, val in enumerate(data_row, start=1):
            ws.cell(row=row_idx, column=col_idx, value=val)

    # Highlight 0s in Comp columns (red) and 1s (green)
    for col_idx, header in enumerate(df_combined.columns, start=1):
        if header.endswith("(Comp)"):
            for row in range(3, ws.max_row + 1):
                cell = ws.cell(row=row, column=col_idx)
                if cell.value == 0:
                    cell.fill = red_fill
                elif cell.value == 1:
                    cell.fill = green_fill

    # Set column widths: filename column wider to ease reading
    for col_idx in range(1, ws.max_column + 1):
        col_letter = get_column_letter(col_idx)
        if col_idx == 1:
            ws.column_dimensions[col_letter].width = 21.5
        else:
            ws.column_dimensions[col_letter].width = 7.5

    center_align = Alignment(horizontal="center", vertical="center")
    for row in ws.iter_rows(min_row=1, max_row=ws.max_row, min_col=1, max_col=ws.max_column):
        for cell in row:
            cell.alignment = center_align


    # Metrics sheet
    df_metrics = df_metrics[["TP", "TN", "FP", "FN", "Accuracy", "Precision", "Recall", "F1-score"]]

    df_metrics.to_excel(writer, sheet_name="Metrics")

    ws_metrics = writer.sheets["Metrics"]
    ws_metrics.sheet_view.showGridLines = False

    start_row = df_metrics.shape[0] + 3
    ws_metrics.cell(row=start_row, column=1, value="Exact Match Ratio")
    ws_metrics.cell(row=start_row, column=2, value=exact_match_ratio)
    ws_metrics.cell(row=start_row+1, column=1, value="Hamming Loss")
    ws_metrics.cell(row=start_row+1, column=2, value=hamming)

    ws_metrics.column_dimensions["A"].width = 24

    # Format columns in Metrics
    max_data_row = df_metrics.shape[0] + 1
    for row in ws_metrics.iter_rows(min_row=2, max_row=max_data_row, min_col=2, max_col=5):
        for cell in row:
            cell.number_format = 'General'

    for row in ws_metrics.iter_rows(min_row=2, max_row=max_data_row, min_col=6, max_col=9):
        for cell in row:
            cell.number_format = '0.00%'

    # Format Exact Match Ratio and Hamming Loss as percentages
    ws_metrics.cell(row=start_row, column=2).number_format = '0.00%'
    ws_metrics.cell(row=start_row+1, column=2).number_format = '0.00%'


    # Summary sheet
    analysis_json_path = results_csv_path / "analysis.json"
    with open(analysis_json_path, "r") as f:
        analysis_data = json.load(f)

    # Pattern to extract subfolder: image_{id}_{subfolder}.jpg
    #   Note: I had to do this because the main execution broke twice and the running
    #   totals and subtotals were lost. Here, I rebuild these values from the detailed
    #   data.
    subfolder_pattern = re.compile(r"image_\d+_(.+)\.jpg", re.IGNORECASE)
    subfolder_data = {}
    totals = {"images": 0, "tokens": 0, "cost": 0.0}

    results_dict = analysis_data.get("analysis_results", {})

    for filename, result in results_dict.items():
        tokens = result.get("tokens_used", 0)
        match = subfolder_pattern.match(filename)
        subfolder = match.group(1) if match else "unknown"

        if subfolder not in subfolder_data:
            subfolder_data[subfolder] = {"images": 0, "tokens": 0, "cost": 0.0}

        subfolder_data[subfolder]["images"] += 1
        subfolder_data[subfolder]["tokens"] += tokens
        subfolder_data[subfolder]["cost"] += tokens * COST_PER_TOKEN

        totals["images"] += 1
        totals["tokens"] += tokens
        totals["cost"] += tokens * COST_PER_TOKEN

    summary_headers = [
        "Subfolder", "Images", "Avg. Tokens per Image", "Avg. Cost per Image", "Tokens Used", "Cost (USD)"
    ]
    summary_rows = []
    for subfolder, stats in subfolder_data.items():
        images = stats["images"]
        tokens = stats["tokens"]
        cost = stats["cost"]
        avg_tokens = tokens / images if images > 0 else 0
        avg_cost = cost / images if images > 0 else 0
        summary_rows.append([
            subfolder,
            images,
            avg_tokens,
            avg_cost,
            tokens,
            cost
        ])

    # Totals row
    avg_tokens_total = totals["tokens"] / totals["images"] if totals["images"] > 0 else 0
    avg_cost_total = totals["cost"] / totals["images"] if totals["images"] > 0 else 0
    totals_row = [
        "Totals",
        totals["images"],
        avg_tokens_total,
        avg_cost_total,
        totals["tokens"],
        totals["cost"]
    ]

    # Write to Excel
    summary_sheet = wb.create_sheet(title="Summary")
    for col_idx, header in enumerate(summary_headers, 1):
        summary_sheet.cell(row=1, column=col_idx, value=header)
    for row_idx, row in enumerate(summary_rows, start=2):
        for col_idx, val in enumerate(row, 1):
            summary_sheet.cell(row=row_idx, column=col_idx, value=val)

    # Totals row
    totals_row_idx = len(summary_rows) + 2
    for col_idx, val in enumerate(totals_row, 1):
        summary_sheet.cell(row=totals_row_idx, column=col_idx, value=val)

    # Session start, end, and duration
    session_start_str = analysis_data.get("session_start")
    session_end_str = analysis_data.get("session_end")

    summary_sheet.cell(row=2, column=8, value="Session Start")
    summary_sheet.cell(row=3, column=8, value="Session End")
    summary_sheet.cell(row=4, column=8, value="Duration")

    if session_start_str and session_end_str:
        try:
            start_dt = datetime.fromisoformat(session_start_str).replace(microsecond=0)
            end_dt = datetime.fromisoformat(session_end_str).replace(microsecond=0)
            duration = end_dt - start_dt

            summary_sheet.cell(row=2, column=9, value=start_dt)
            summary_sheet.cell(row=3, column=9, value=end_dt)
            summary_sheet.cell(row=4, column=9, value=str(duration))

            summary_sheet.cell(row=2, column=9).number_format = 'MMM DD, YYYY HH:MM:SS'
            summary_sheet.cell(row=3, column=9).number_format = 'MMM DD, YYYY HH:MM:SS'
        except ValueError:
            pass

    # Format columns and adjust widths
    for row in range(2, totals_row_idx + 1):
        summary_sheet[f"C{row}"].number_format = '0.0'
        summary_sheet[f"D{row}"].number_format = '"$"#,##0.0000'
        summary_sheet[f"F{row}"].number_format = '"$"#,##0.0000'
    summary_sheet.column_dimensions["A"].width = 18
    summary_sheet.column_dimensions["B"].width = 10
    summary_sheet.column_dimensions["C"].width = 18
    summary_sheet.column_dimensions["D"].width = 16
    summary_sheet.column_dimensions["E"].width = 15
    summary_sheet.column_dimensions["F"].width = 12

    console.print("[bold]Subfolder Summary:[/bold]")
    table = Table(show_header=True, header_style="bold magenta")
    for h in summary_headers:
        table.add_column(h)
    for row in summary_rows:
        table.add_row(
            str(row[0]), str(row[1]), f"{row[2]:.1f}", f"${row[3]:.6f}", str(row[4]), f"${row[5]:.6f}"
        )

    table.add_row(
        str(totals_row[0]), str(totals_row[1]), f"{totals_row[2]:.1f}", f"${totals_row[3]:.6f}",
        str(totals_row[4]), f"${totals_row[5]:.6f}"
    )

    console.print(table)

total_time = time.time() - start_time
console.print("\n[bold]Accuracy per label:[/bold]")
for k, v in accuracy.items():
    console.print(f"  {k}: {v:.2%}")

console.print(f"\nExact Match Ratio: {exact_match_ratio:.2%}")
console.print(f"Hamming Loss: {hamming:.2%}")

console.print(f"\nProcessed {len(merged)} images", style="bold cyan")
console.print()

console.print(f"Subfolder processing time: {time.time() - subfolder_start:.2f} seconds", style="cyan")
console.print(f"Total processing time: {total_time:.2f} seconds", style="cyan")
console.print()
